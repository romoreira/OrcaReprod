{
    "receiver":"webhook",
    "status":"firing",
    "alerts":[
       {
          "status":"resolved",
          "labels":{
             "alertname":"KubeCPUOvercommit",
             "prometheus":"monitoring/prometheus-prometheus-oper-prometheus",
             "severity":"warning"
          },
          "annotations":{
             "message":"Cluster has overcommitted CPU resource requests for Pods and cannot tolerate node failure.",
             "runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit"
          },
          "startsAt":"2020-01-24T14:03:28.714203701Z",
          "endsAt":"2020-01-24T14:23:28.714203701Z",
          "generatorURL":"http://prometheus-prometheus-oper-prometheus.monitoring:9090/graph?g0.expr=sum%28namespace%3Akube_pod_container_resource_requests_cpu_cores%3Asum%29+%2F+sum%28kube_node_status_allocatable_cpu_cores%29+%3E+%28count%28kube_node_status_allocatable_cpu_cores%29+-+1%29+%2F+count%28kube_node_status_allocatable_cpu_cores%29&g0.tab=1",
          "fingerprint":"e6a9cb6836bb0283"
       },
       {
          "status":"firing",
          "labels":{
             "alertname":"Watchdog",
             "prometheus":"monitoring/prometheus-prometheus-oper-prometheus",
             "severity":"none"
          },
          "annotations":{
             "message":"This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n"
          },
          "startsAt":"2020-01-24T13:57:26.904694511Z",
          "endsAt":"0001-01-01T00:00:00Z",
          "generatorURL":"http://prometheus-prometheus-oper-prometheus.monitoring:9090/graph?g0.expr=vector%281%29&g0.tab=1",
          "fingerprint":"eec32a2c7b3e6ab0"
       },
       {
          "status":"resolved",
          "labels":{
             "alertname":"KubePodNotReady",
             "namespace":"default",
             "pod":"curler2-66f66bfc5b-x6tmv",
             "prometheus":"monitoring/prometheus-prometheus-oper-prometheus",
             "severity":"critical"
          },
          "annotations":{
             "message":"Pod default/curler2-66f66bfc5b-x6tmv has been in a non-ready state for longer than 15 minutes.",
             "runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready"
          },
          "startsAt":"2020-01-24T14:13:14.021059511Z",
          "endsAt":"2020-01-24T14:22:44.021059511Z",
          "generatorURL":"http://prometheus-prometheus-oper-prometheus.monitoring:9090/graph?g0.expr=sum+by%28namespace%2C+pod%29+%28max+by%28namespace%2C+pod%29+%28kube_pod_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cphase%3D~%22Pending%7CUnknown%22%7D%29+%2A+on%28namespace%2C+pod%29+group_left%28owner_kind%29+max+by%28namespace%2C+pod%2C+owner_kind%29+%28kube_pod_owner%7Bowner_kind%21%3D%22Job%22%7D%29%29+%3E+0&g0.tab=1",
          "fingerprint":"b63ec5440245d4a1"
       },
       {
          "status":"resolved",
          "labels":{
             "alertname":"KubePodNotReady",
             "namespace":"default",
             "pod":"curler2-766b675df6-d4gv2",
             "prometheus":"monitoring/prometheus-prometheus-oper-prometheus",
             "severity":"critical"
          },
          "annotations":{
             "message":"Pod default/curler2-766b675df6-d4gv2 has been in a non-ready state for longer than 15 minutes.",
             "runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready"
          },
          "startsAt":"2020-01-24T14:13:14.021059511Z",
          "endsAt":"2020-01-24T14:22:44.021059511Z",
          "generatorURL":"http://prometheus-prometheus-oper-prometheus.monitoring:9090/graph?g0.expr=sum+by%28namespace%2C+pod%29+%28max+by%28namespace%2C+pod%29+%28kube_pod_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cphase%3D~%22Pending%7CUnknown%22%7D%29+%2A+on%28namespace%2C+pod%29+group_left%28owner_kind%29+max+by%28namespace%2C+pod%2C+owner_kind%29+%28kube_pod_owner%7Bowner_kind%21%3D%22Job%22%7D%29%29+%3E+0&g0.tab=1",
          "fingerprint":"9fd32cc7b01b5d6c"
       },
       {
          "status":"resolved",
          "labels":{
             "alertname":"KubePodNotReady",
             "namespace":"default",
             "pod":"curler4-5c89bd97f6-dm2dk",
             "prometheus":"monitoring/prometheus-prometheus-oper-prometheus",
             "severity":"critical"
          },
          "annotations":{
             "message":"Pod default/curler4-5c89bd97f6-dm2dk has been in a non-ready state for longer than 15 minutes.",
             "runbook_url":"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready"
          },
          "startsAt":"2020-01-24T14:13:14.021059511Z",
          "endsAt":"2020-01-24T14:22:44.021059511Z",
          "generatorURL":"http://prometheus-prometheus-oper-prometheus.monitoring:9090/graph?g0.expr=sum+by%28namespace%2C+pod%29+%28max+by%28namespace%2C+pod%29+%28kube_pod_status_phase%7Bjob%3D%22kube-state-metrics%22%2Cphase%3D~%22Pending%7CUnknown%22%7D%29+%2A+on%28namespace%2C+pod%29+group_left%28owner_kind%29+max+by%28namespace%2C+pod%2C+owner_kind%29+%28kube_pod_owner%7Bowner_kind%21%3D%22Job%22%7D%29%29+%3E+0&g0.tab=1",
          "fingerprint":"637f6df5164378f1"
       }
    ],
    "groupLabels":{

    },
    "commonLabels":{
       "prometheus":"monitoring/prometheus-prometheus-oper-prometheus"
    },
    "commonAnnotations":{

    },
    "externalURL":"http://prometheus-prometheus-oper-alertmanager.monitoring:9093",
    "version":"4",
    "groupKey":"{}:{}"
 }
